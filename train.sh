#!/bin/bash

# agt 9 = DQN
# usr 1 = user rule simulator

python -m ddq \
    --agt 9 \
    --usr 1 \
    \
    --episodes 10 \
    --warm_start_epochs 100 \
    --max_turn 40 \
    --simulation_epoch_size 10 \
    \
    --movie_kb_path assets/corpus/movies.unix.pkl \
    --goal_file_path assets/corpus/goals.unix.pkl \
    --write_model_dir assets/agents/ddq \
    --dict_path assets/corpus/vocabulary.unix.pkl \
    --nlg_model_path assets/models/nlg/lstm-tanh-relu.unix.pkl \
    --nlu_model_path assets/models/nlu/lstm.unix.pkl \
    --act_set assets/corpus/dialogue-act-set.txt \
    --slot_set assets/corpus/slot-set.txt \
    --diaact_nl_pairs assets/corpus/dialogue-act-nl-pairs.json \
    \
    --dqn_hidden_size 80 \
    --batch_size 16 \
    --experience_replay_pool_size 5000 \
    --run_mode 1 \
    --act_level 0 \
    --slot_err_prob 0.0 \
    --intent_err_prob 0.00 \
    --warm_start 1 \
    --planning_steps 10 \
    \
    --torch_seed 100 \
    --grounded 0 \
    --boosted 1 \
    --train_world_model 1

# python run.py \
#     --agt 9 \  # agent id
#     --usr 1 \  # user simulator id
#     \
#     --episodes 500 \  # maximum number of dialogues?
#     --max_turn 40 \  # maximum number of dialogue turns
#     --simulation_epoch_size 100 \  # how many dialogues to simulate during epoch
#     \
#     --movie_kb_path ./deep_dialog/data/movie_kb.1k.p \  # knowledge base path
#     --goal_file_path ./deep_dialog/data/user_goals_first_turn_template.part.movie.v1.p \
#     --write_model_dir ./deep_dialog/checkpoints/DDQAgent \
#     \
#     --dqn_hidden_size 80 \  # model hidden size
#     --batch_size 16 \
#     --experience_replay_pool_size 5000 \
#     --run_mode 3 \  # 3 means no disaplay which is set for training
#     --act_level 0 \  # 0 = Dia Act level, 1 = NL level
#     --slot_err_prob 0.0 \
#     --intent_err_prob 0.00 \
#     --warm_start 1 \  # use rule policy to fill the experimence replay buffer
#     --warm_start_epochs 100 \
#     --planning_steps K-1 \
#     \
#     --torch_seed 100 \
#     --grounded 0 \  # planning steps with environment rather than world model
#     --boosted 1 \  # boost the world model by examples generated by the rule agent
#     --train_world_model 1  # train world model on the fly
