#!/bin/bash

# agt 9 = DQN
# usr 1 = user rule simulator

python run.py \
    --agt 9 \
    --usr 1 \
    \
    --episodes 10 \
    --max_turn 40 \
    --simulation_epoch_size 10 \
    \
    --movie_kb_path ./deep_dialog/data/movie_kb.1k.lf.p \
    --goal_file_path ./deep_dialog/data/user_goals_first_turn_template.part.movie.v1.lf.p \
    --write_model_dir ./deep_dialog/checkpoints/DDQAgent \
    --dict_path ./deep_dialog/data/dicts.v3.lf.p \
    --nlg_model_path ./deep_dialog/models/nlg/lstm_tanh_relu_[1468202263.38]_2_0.610.lf.p \
    --nlu_model_path ./deep_dialog/models/nlu/lstm_[1468447442.91]_39_80_0.921.lf.p \
    \
    --dqn_hidden_size 80 \
    --batch_size 16 \
    --experience_replay_pool_size 5000 \
    --run_mode 3 \
    --act_level 0 \
    --slot_err_prob 0.0 \
    --intent_err_prob 0.00 \
    --warm_start 1 \
    --warm_start_epochs 100 \
    --planning_steps 10 \
    \
    --torch_seed 100 \
    --grounded 0 \
    --boosted 1 \
    --train_world_model 1

# python run.py \
#     --agt 9 \  # agent id
#     --usr 1 \  # user simulator id
#     \
#     --episodes 500 \  # maximum number of dialogues?
#     --max_turn 40 \  # maximum number of dialogue turns
#     --simulation_epoch_size 100 \  # how many dialogues to simulate during epoch
#     \
#     --movie_kb_path ./deep_dialog/data/movie_kb.1k.p \  # knowledge base path
#     --goal_file_path ./deep_dialog/data/user_goals_first_turn_template.part.movie.v1.p \
#     --write_model_dir ./deep_dialog/checkpoints/DDQAgent \
#     \
#     --dqn_hidden_size 80 \  # model hidden size
#     --batch_size 16 \
#     --experience_replay_pool_size 5000 \
#     --run_mode 3 \  # 3 means no disaplay which is set for training
#     --act_level 0 \  # 0 = Dia Act level, 1 = NL level
#     --slot_err_prob 0.0 \
#     --intent_err_prob 0.00 \
#     --warm_start 1 \  # use rule policy to fill the experimence replay buffer
#     --warm_start_epochs 100 \
#     --planning_steps K-1 \
#     \
#     --torch_seed 100 \
#     --grounded 0 \  # planning steps with environment rather than world model
#     --boosted 1 \  # boost the world model by examples generated by the rule agent
#     --train_world_model 1  # train world model on the fly
