# Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning
*An implementation of the  
[Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning](https://arxiv.org/abs/1801.06176)*

This document describes how to run the simulation of DDQ Agent.

## Content
* [Data](#data)
* [Parameter](#parameter)
* [Running Dialogue Agents](#running-dialogue-agents)
* [Evaluation](#evaluation)
* [Reference](#reference)
* [Notes](#notes)

## Data
all the data is under this folder: ./src/deep_dialog/data

* Movie Knowledge Bases<br/>
`movie_kb.1k.p` --- 94% success rate (for `user_goals_first_turn_template_subsets.v1.p`)<br/>
`movie_kb.v2.p` --- 36% success rate (for `user_goals_first_turn_template_subsets.v1.p`)

The former contains 991 items, for example:

```python
{
    'city': 'hamilton',
    'theater': 'manville 12 plex',
    'zip': '08835',
    'critic_rating': 'good',
    'date': 'tomorrow',
    'state': 'nj',
    'starttime': '10:30am',
    'genre': 'comedy',
    'moviename': 'zootopia'
}
```

* User Goals<br/>
`user_goals_first_turn_template.v2.p` --- user goals extracted from the first user turn<br/>
`user_goals_first_turn_template.part.movie.v1.p` --- a subset of user goals [Please use this one, the upper bound success rate on movie_kb.1k.json is 0.9765.]

The latter contains 128 items, for example:

```python
{                                                                                                                                         
  "request_slots": {
    "starttime": "UNK",
    "ticket": "UNK"
  },
  "diaact": "request",
  "inform_slots": {
    "date": "tomorrow",
    "moviename": "risen",
    "numberofpeople": "2",
    "theater": "regency commerce 14"
  }
}
```

* NLG Rule Template<br/>
`dia_act_nl_pairs.v6.json` --- some predefined NLG rule templates for both User simulator and Agent.

* Dialog Act Intent<br/>
`dia_acts.txt`

* Dialog Act Slot<br/>
`slot_set.txt`

* Movie Dict

`./deep_dialog/data/dicts.v3.p` --- apparently contains slot vocabularies.

```python
(
    'city',
    'numberofpeople',
    'theater',
    'description',
    'zip',
    'numberofkids',
    'distanceconstraints',
    'critic_rating',
    'price',
    'greeting',
    'actor',
    'date',
    'state',
    'other',
    'mpaa_rating',
    'starttime',
    'theater_chain',
    'genre',
    'video_format',
    'moviename'
)
```

## Parameter

### Basic setting

`--agt`: the agent id<br/>
`--usr`: the user (simulator) id<br/>
`--max_turn`: maximum turns<br/>
`--episodes`: how many dialogues to run<br/>
`--slot_err_prob`: slot level err probability<br/>
`--slot_err_mode`: which kind of slot err mode<br/>
`--intent_err_prob`: intent level err probability

### DDQ Agent setting
`--grounded`: planning k steps with environment rather than world model, serving as a upper bound.<br/>
`--boosted`: boost the world model with examples generated by rule agent<br/>
`--train_world_model`: train world model on the fly<br/>

### Data setting

`--movie_kb_path`: the movie kb path for agent side<br/>
`--goal_file_path`: the user goal file path for user simulator side

### Model setting

`--dqn_hidden_size`: hidden size for RL agent<br/>
`--batch_size`: batch size for DDQ training<br/>
`--simulation_epoch_size`: how many dialogue to be simulated in one epoch<br/>
`--warm_start`: use rule policy to fill the experience replay buffer at the beginning<br/>
`--warm_start_epochs`: how many dialogues to run in the warm start

### Display setting

`--run_mode`: 0 for display mode (NL); 1 for debug mode (Dia_Act); 2 for debug mode (Dia_Act and NL); >3 for no display (i.e. training)<br/>
`--act_level`: 0 for user simulator is Dia_Act level; 1 for user simulator is NL level<br/>
`--auto_suggest`: 0 for no auto_suggest; 1 for auto_suggest<br/>
`--cmd_input_mode`: 0 for NL input; 1 for Dia_Act input. (this parameter is for AgentCmd only)

### Others

`--write_model_dir`: the directory to write the models<br/>
`--trained_model_path`: the path of the trained RL agent model; load the trained model for prediction purpose.

`--learning_phase`: train/test/all, default is all. You can split the user goal set into train and test set, or do not split (all); We introduce some randomness at the first sampled user action, even for the same user goal, the generated dialogue might be different.<br/>

## Running Dialogue Agents

Before running training create the required folders:

```sh
mkdir -p src/deep_dialog/checkpoints/DDQAgent
```

If you run the system on linux you will also need to convert pickle files from dos format (by replacing `CRLF` with `LF`):

```sh
./dos2unix.sh
```

Then training can be started through `src/run.sh` script:

```sh
./run.sh
```

Train DDQ Agent with K planning steps:
```sh
python run.py --agt 9 --usr 1 --max_turn 40 
	      --movie_kb_path ./deep_dialog/data/movie_kb.1k.p 
	      --dqn_hidden_size 80 --experience_replay_pool_size 5000 
	      --episodes 500 
	      --simulation_epoch_size 100 
	      --run_mode 3 
	      --act_level 0 
	      --slot_err_prob 0.0 
	      --intent_err_prob 0.00 
	      --batch_size 16 
	      --goal_file_path ./deep_dialog/data/user_goals_first_turn_template.part.movie.v1.p 
	      --warm_start 1 --warm_start_epochs 100 
	      --planning_steps K-1 
	      --write_model_dir ./deep_dialog/checkpoints/DDQAgent
	      --torch_seed 100
	      --grounded 0
	      --boosted 1
	      --train_world_model 1

```



Test RL Agent with N dialogues:
```sh
python run.py --agt 9 --usr 1 --max_turn 40
	      --movie_kb_path ./deep_dialog/data/movie_kb.1k.p
	      --dqn_hidden_size 80
	      --experience_replay_pool_size 1000
	      --episodes 300 
	      --simulation_epoch_size 100
	      --write_model_dir ./deep_dialog/checkpoints/DDQAgent/
	      --slot_err_prob 0.00
	      --intent_err_prob 0.00
	      --batch_size 16
	      --goal_file_path ./deep_dialog/data/user_goals_first_turn_template.part.movie.v1.p
	      --trained_model_path ./deep_dialog/checkpoints/DDQAgent/TRAINED_MODEL
	      --run_mode 3
```
## Experiments
To run the scripts, move the two bash files under src folder. 
1. Bash_figure_4.sh is the script for figure 4.
2. Bash_figure_5.sh is the script for figure 5. 

## Evaluation
To evaluate the performance of agents, three metrics are available: success rate, average reward, average turns. Here we show the learning curve with success rate.

1. Plotting Learning Curve
`python draw_learning_curve.py --result_file ./deep_dialog/checkpoints/DDQAgent/noe2e/TRAINED_MODEL.json`
2. Pull out the numbers and draw the curves in Excel

## Reference

Main papers to be cited
```
@inproceedings{Peng2018DeepDynaQ,
  title={Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning},
  author={Peng, Baolin and Li, Xiujun and Gao, Jianfeng and Liu, Jingjing and Wong, Kam-Fai and Su, Shang-Yu},
  booktitle={ACL},
  year={2018}
}

@article{li2016user,
  title={A User Simulator for Task-Completion Dialogues},
  author={Li, Xiujun and Lipton, Zachary C and Dhingra, Bhuwan and Li, Lihong and Gao, Jianfeng and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:1612.05688},
  year={2016}
}
```

## Notes

List of dialogue acts is provided in the file `src/deep_dialog/data/dia_acts.txt`:

```
request
inform
confirm_question
confirm_answer
greeting
closing
multiple_choice
thanks
welcome
deny
not_sure
```

List of slots is provided in the file `src/deep_dialog/data/slot_set.txt`:

```
actor
actress
city
closing
critic_rating
date
description
distanceconstraints
genre
greeting
implicit_value
movie_series
moviename
mpaa_rating
numberofpeople
numberofkids
taskcomplete
other
price
seating
starttime
state
theater
theater_chain
video_format
zip
result
ticket
mc_list
```

The main module works according to the activity diagram:

[![](https://mermaid.ink/img/pako:eNqFVstunDAU_RWLZZVp1agrFGVR5Q_aXaksF-4wVow99WPSJJp_77UN2BiYsIJzju_jXGN4r1rVQVVXxjILT5z1mg2Hy30jO66htVxJ8vN7IxtpzoJb2ismTE3CA_lKvpBvXslYa6kU9My4RhYBoXoHBGEiBQmwj9EKZswTHEmr9NkZYqxWz1D3GiBnL6plf5xg-nVSaOgyfsCSxURduBJgffRfn36Tw-GRxBrrOiZJ-KAuHLYILNPDKW2ijFC7XML2FoOlBnzpEyKFQ22oPwf7NViY6vsLfQWSZMPwTOwsUFwiwXqQaInvawWGjtboWOqKaGQ2jZl0BnQZP2JF-BFM5ZbUdvwXpUVHR0uWaRZUkW3JbSVdKBqJ1icOH1abuWD9CpdjzmMTvzJvTcSm13hR1xR9L6LbiehuRwxBkiS-p3RgEokpiI_3gSQLva8s51byxehKutzS6_AbcJBrJymcucHywssRTjbCj2P3Dw_tSfEWHh9Lit5vk3m7uWCS5AnJeyMJXnM6X9EL0wNFrcZ3jA-41f2xWqOGdH8lGSe7WnVUmgJrT1Nsn9CLNqPdXBHLLNmpVH8tbfzsrUWEv4EXv3lx6OPEBaARGqh1Wpq0Pt3FXKUyz7WRT8I_G4RJdW2yh1W4sC3mseXSbJhe5OVUSXrGLwgPn7MwyC3z_bUjj6f5bDaaotBHb6jVDM2SfWbALVlW0_FYZinLWCvm_Z2SpEVLPEjducNp5FHwm650Z3LHdkWxWF85LW1K4DSKxXGTDaM8K-LSDA2jMCflRBdJEkgyn1zLpGU8wy5Qpi3LyzXZoGY0idAEHBduyxaSVeN2vFZ31QBI8g7_l8KGbip7ggGaqsbbjunnpoo65qz68Srbqj7i5wfuqmjy-H81otf_RWVdzw?type=png)](https://mermaid.live/edit#pako:eNqFVstunDAU_RWLZZVp1agrFGVR5Q_aXaksF-4wVow99WPSJJp_77UN2BiYsIJzju_jXGN4r1rVQVVXxjILT5z1mg2Hy30jO66htVxJ8vN7IxtpzoJb2ismTE3CA_lKvpBvXslYa6kU9My4RhYBoXoHBGEiBQmwj9EKZswTHEmr9NkZYqxWz1D3GiBnL6plf5xg-nVSaOgyfsCSxURduBJgffRfn36Tw-GRxBrrOiZJ-KAuHLYILNPDKW2ijFC7XML2FoOlBnzpEyKFQ22oPwf7NViY6vsLfQWSZMPwTOwsUFwiwXqQaInvawWGjtboWOqKaGQ2jZl0BnQZP2JF-BFM5ZbUdvwXpUVHR0uWaRZUkW3JbSVdKBqJ1icOH1abuWD9CpdjzmMTvzJvTcSm13hR1xR9L6LbiehuRwxBkiS-p3RgEokpiI_3gSQLva8s51byxehKutzS6_AbcJBrJymcucHywssRTjbCj2P3Dw_tSfEWHh9Lit5vk3m7uWCS5AnJeyMJXnM6X9EL0wNFrcZ3jA-41f2xWqOGdH8lGSe7WnVUmgJrT1Nsn9CLNqPdXBHLLNmpVH8tbfzsrUWEv4EXv3lx6OPEBaARGqh1Wpq0Pt3FXKUyz7WRT8I_G4RJdW2yh1W4sC3mseXSbJhe5OVUSXrGLwgPn7MwyC3z_bUjj6f5bDaaotBHb6jVDM2SfWbALVlW0_FYZinLWCvm_Z2SpEVLPEjducNp5FHwm650Z3LHdkWxWF85LW1K4DSKxXGTDaM8K-LSDA2jMCflRBdJEkgyn1zLpGU8wy5Qpi3LyzXZoGY0idAEHBduyxaSVeN2vFZ31QBI8g7_l8KGbip7ggGaqsbbjunnpoo65qz68Srbqj7i5wfuqmjy-H81otf_RWVdzw)

Warmup activity diagram:

[![](https://mermaid.ink/img/pako:eNp9U81uwyAMfhXEcWovO-aww9Q32G5jQl5wU1QwkSHpqqrvPkKTNV3TcQr-_pzYOck6GJSVjAkSbiw0DH7dPysylrFONpB4f1WkqHYQ4wa3og7cdlHExGGPVcOIc7QPNXx1Dvg4MRjNDPc5zk1Qb4PDNLh_PH2K9fpFWLJJY2tjpima3wp82FmH2gdGnTqmOEhL5zdCcVIk8ong20xvAriiHu9ZwElDeTlF56vHX_fJh_A7lcpALcaF_lueeMMJhHoXcis0fFjdRWR9oc86gAbp2sEkXQCLiLGxMWWfZdkiPE8rPTwIm2G3WYuiJXSU5a0wOjFY0gYS6G0YG5qrH5L-MRmiLh7ncVh3YxrEF9UhsDO67Jiiu1Ih5k2TK-mRPViTF7_MTsm0Q49KVvnRAO-VzGGZB10Kb0eqZbUFF3Elu9Zcf5Sxev4BirYuOw?type=png)](https://mermaid.live/edit#pako:eNp9U81uwyAMfhXEcWovO-aww9Q32G5jQl5wU1QwkSHpqqrvPkKTNV3TcQr-_pzYOck6GJSVjAkSbiw0DH7dPysylrFONpB4f1WkqHYQ4wa3og7cdlHExGGPVcOIc7QPNXx1Dvg4MRjNDPc5zk1Qb4PDNLh_PH2K9fpFWLJJY2tjpima3wp82FmH2gdGnTqmOEhL5zdCcVIk8ong20xvAriiHu9ZwElDeTlF56vHX_fJh_A7lcpALcaF_lueeMMJhHoXcis0fFjdRWR9oc86gAbp2sEkXQCLiLGxMWWfZdkiPE8rPTwIm2G3WYuiJXSU5a0wOjFY0gYS6G0YG5qrH5L-MRmiLh7ncVh3YxrEF9UhsDO67Jiiu1Ih5k2TK-mRPViTF7_MTsm0Q49KVvnRAO-VzGGZB10Kb0eqZbUFF3Elu9Zcf5Sxev4BirYuOw)

More [notes here](Notes.md).
