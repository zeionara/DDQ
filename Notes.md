# Rule-based user simulation

The data generated during simulation is structured as follows:

## User

By default simulated by `RuleSimulator`.  
When `RuleSimulator` observes `taskcomplete` field in the `agent action` it answers with `thanks` dialog act marking the end of conversation.

User has a **goal**:

1. `diaact` - dialog act (for example, `request` or `thanks`);
1. `inform_slots` - slots which the user has information about (for example, `theater` or `moviename`);
1. `request_slots` - slots which the user asks the agent about (for example, `starttime` or `ticket`).

User has a **state**:

1. `inform_slots` - subset of slots from the `goal.inform_slots`;
1. `rest_slots` = `goal.inform_slots` - `user.inform_slots` + `goal.request_slots`;
1. `request_slots` - subset of slots from the `goal.request_slots`;
1. etc.

If at the end of dialog `state.request_slots` or `state.rest_set` contain some items, then dialog is considered to be failed.

User can sample **actions**:

1. `diaact` - see `goal.diaact`;
1. `turn` - index of dialog turn (for example, `0` or `10`);
1. `inform_slots` - see `user.inform_slots`;
1. `request_slots` - see `user.request_slots`;
1. `nl` - representation of an action in a form of natural language (for example, `What is the start time for risen at regency commerce 14?`, generated by `nlg` model).

## StateTracker

StateTracker has **history**, which is array of dicts:

1. `diaact` - see `user.action.diaact`;
1. `turn` - see `user.action.turn`;
1. `inform_slots` - see `user.action.inform_slots`;
1. `request_slots` - see `user.action.request_slots`;
1. `speaker` - source of an utterance (for example, `user` or `agent`).

StateTracker has **state for agent**:

1. `user_action` - last item from the `history`;
1. `current_slots` - running record of slots which are filled and requested:
    * `inform_slots` - slots which are filled by user;
    * `request_slots` - slots which are requested by user;
    * `proposed_slots` - slots which are filled by agent;
    * `agent_request_slots` - slots which are requested by agent.
1. `kb_results_dict` - additional data about filled slots:
    * for each `slot` from `current_slots.inform_slots` - hard-coded value `0` (but should be number of oresults matching each current constraint);
    * `matching_all_constraints` - hard-coded value `0` (but should be number of results matching each current constraint from `current_slots.inform_slots`);
1. `turn` - current dialog turn index;
1. `history` - see `history`;
1. `agent_action` - last agent action or `None` (if it is the first state that is passed to agent).

StateTracker inserts all `current` `inform_slots` when given an `agent action` with `taskcomplete` key which happens when `agent` triend to ask about every feasible slot and hopefully gathered all data (except data that user is asking for, which they will not disclose during the conversation even when explicitly asked by agent about it).

## Agent (DQN)

Agent represents input data (including `user.action` and its own last action) as stacked vectors:

1. `diaact` - one-hot encoded `user.action.diaact`;
1. `inform_slots` - many-hot encoded `user.action.inform_slots` **names**;
1. `request_slots` - many-hot encoded `user.action.request_slots` **names**;
1. `current_inform_slots` - many-hot encoded `current_slots.inform_slots` **names**;
1. `diaact` - one-hot encoded `last_agent_action.diaact` from `state_for_agent`;
1. `agent_inform_slots` - many-hot encoded `last_agent_action.inform_slots` from `state_for_agent`;
1. `agent_request_slots` - many-hot encoded `last_agent_action.request_slots` from `state_for_agent`;
1. `turn_onehot_rep` - one-hot encoded `turn` index from `state_for_agent` (current turn index);
1. `turn_onehot_rep` - one-hot encoded `turn` index from `state_for_agent`;
1. `kb_count_rep` - hard-coded array of zeros with number of items equal to the slot vocabulary size + 1;
1. `kb_binary_rep` - hard-coded array of zeros with number of items equal to the slot vocabulary size + 1.

Agent implements and epsilon-greedy policy:
1. Can sample a random action;
1. Can use model to generate an action (an action is defined by `diaact` and `slots`, in total there are 29 feasible actions):
    1. Rule-based model - applied on warmup step:
        1. Ask every askable slot one by one;
        1. Send `inform` response;
        1. Send `thanks` response.
    1. Network-based model (sample action from the neural model).

Agents fills an **experience replay pool** during training stage which is a list of tuples:
1. `state_representation @ t` - input data representation by the agent (see the first list above) for the **step before generating the last user action**;
1. `action` - last action generated by the agent (see the second list above);
1. `reward` - reward for the last action generated by the agent;
1. `state_representation @ t + 1` - input data representation by the agent (see the first list above) for the **step after generating the last user action**;
1. `episode_over` - is the last turn in the episode;
1. `user_state_representation @ t + 1` - see `state_representation @ t + 1`.

During warmup stage the world model fills a similar experience replay pool.

### Agent failures

There are many reasons for the rule-based agent to fail:
1. The only way to get information about something is asking user abount it. No external knowledge source, when asking about data which is users asking about themselves, there is nowhere to get the required data from;
1. There are slots which the user cares about, but which are missing the agent's predefined list of slots for asking user.

## Reward

Reward is computed after every user action except the first one and can accept three values:

1. `-1` if dialog is not completed;
1. `-max_turn` if dialog completed with failure;
1. `2 * max_turn` if dialog completed with success.

Reward without penalty - the same as usual reward, but instead of negative values zero (`0`) is generated.
